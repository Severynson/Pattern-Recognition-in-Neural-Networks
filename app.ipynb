{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Jupyter notebook demonstrates the creation of a simple neural network for educational purposes. The goal is to make the neural network easily interpretable for beginners and help develop a better basic intuition of what complex, thousand-unit multilayer neural networks are doing in a nutshell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# 1. DATASET: 5x3 \"pictures\" of digits 0–9 (binary pixels)\n",
    "# ============================================================\n",
    "\n",
    "digits = np.array([\n",
    "    # 0\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [1,0,1],\n",
    "        [1,0,1],\n",
    "        [1,0,1],\n",
    "        [1,1,1],\n",
    "    ],\n",
    "    # 1\n",
    "    [\n",
    "        [0,0,1],\n",
    "        [0,0,1],\n",
    "        [0,0,1],\n",
    "        [0,0,1],\n",
    "        [0,0,1],\n",
    "    ],\n",
    "    # 2\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [0,0,1],\n",
    "        [1,1,1],\n",
    "        [1,0,0],\n",
    "        [1,1,1],\n",
    "    ],\n",
    "    # 3\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [0,0,1],\n",
    "        [1,1,1],\n",
    "        [0,0,1],\n",
    "        [1,1,1],\n",
    "    ],\n",
    "    # 4\n",
    "    [\n",
    "        [1,0,1],\n",
    "        [1,0,1],\n",
    "        [1,1,1],\n",
    "        [0,0,1],\n",
    "        [0,0,1],\n",
    "    ],\n",
    "    # 5\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [1,0,0],\n",
    "        [1,1,1],\n",
    "        [0,0,1],\n",
    "        [1,1,1],\n",
    "    ],\n",
    "    # 6\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [1,0,0],\n",
    "        [1,1,1],\n",
    "        [1,0,1],\n",
    "        [1,1,1],\n",
    "    ],\n",
    "    # 7\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [0,0,1],\n",
    "        [0,1,1],\n",
    "        [0,0,1],\n",
    "        [0,0,1],\n",
    "    ],\n",
    "    # 8\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [1,0,1],\n",
    "        [1,1,1],\n",
    "        [1,0,1],\n",
    "        [1,1,1],\n",
    "    ],\n",
    "    # 9\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [1,0,1],\n",
    "        [1,1,1],\n",
    "        [0,0,1],\n",
    "        [1,1,1],\n",
    "    ],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.arange(10)    # 0..9\n",
    "\n",
    "num_samples = digits.shape[0]\n",
    "height, width = digits.shape[1], digits.shape[2]\n",
    "input_size = height * width  # 5 * 3 = 15\n",
    "\n",
    "# Flatten each 5x3 picture into a vector of length 15\n",
    "X = digits.reshape(num_samples, input_size)  # shape: (10, 15)\n",
    "y = labels                                   # shape: (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_digit(digit_matrix):\n",
    "    \"\"\"Display a 5x3 digit image.\"\"\"\n",
    "    fig = plt.figure(facecolor='black')\n",
    "    plt.imshow(digit_matrix, cmap='gray', interpolation='nearest', vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPEAAAGFCAYAAAAsDxVqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEvElEQVR4nO3aMU4AMRAEQRvd/7+8PAEEEqs+qmIHk7Q28T3nzAGyPrYHAL8jYogTMcSJGOJEDHEihjgRQ5yIIe757sMZf0Lgr917v3zjEkOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUPcsz3gbe692xMSZmZ7wmu4xBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBD3bA94m5nZnsA/4xJDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFD3LM94G3uvdsTEmZme8JruMQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQ92wPeJuZ2Z7AP+MSQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ9w958z2CODnXGKIEzHEiRjiRAxxIoY4EUOciCFOxBAnYoj7BKZvFAsRamiCAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_digit(digits[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_recognizing_right_edge = np.array([\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "])\n",
    "\n",
    "neuron_recognizing_3_horizontal_stripes = np.array([\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 1],\n",
    "])\n",
    "\n",
    "neuron_recognizing_middle_dot = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "])\n",
    "\n",
    "neuron_recognizing_2nd_row_leftmost_pixel = np.array([\n",
    "    [0, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "])\n",
    "\n",
    "neuron_recognizing_2nd_row_rightmost_pixel = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "])\n",
    "\n",
    "neuron_recognizing_4th_row_leftmost_pixel = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 0],\n",
    "])\n",
    "\n",
    "neuron_recognizing_4th_row_rightmost_pixel = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 0],\n",
    "])\n",
    "\n",
    "hidden_masks = [\n",
    "    neuron_recognizing_3_horizontal_stripes,\n",
    "    neuron_recognizing_middle_dot,\n",
    "    neuron_recognizing_right_edge,\n",
    "    neuron_recognizing_2nd_row_leftmost_pixel,\n",
    "    neuron_recognizing_2nd_row_rightmost_pixel,\n",
    "    neuron_recognizing_4th_row_leftmost_pixel,\n",
    "    neuron_recognizing_4th_row_rightmost_pixel,\n",
    "]\n",
    "\n",
    "hidden_size_manual = len(hidden_masks)\n",
    "\n",
    "# Each mask becomes a 15-long weight vector\n",
    "W_hand = np.stack([m.reshape(-1) for m in hidden_masks], axis=0)  # (7, 15)\n",
    "b_hand = np.zeros(hidden_size_manual)  # no bias for these feature neurons\n",
    "\n",
    "\n",
    "# === Your original output-layer \"rules\" turned into matrices ===\n",
    "# Each [w, b] pair becomes:\n",
    "#   - w  -> weight from that hidden feature\n",
    "#   - all b's are summed into one bias term for that output neuron\n",
    "\n",
    "neuron_classifying_0 = [\n",
    "    [1, 0], [-100, 0], [0, 0], [1, 0], [1, 0], [1, 0], [1, 0],\n",
    "]\n",
    "\n",
    "neuron_classifying_1 = [\n",
    "    [-1, 0], [-1000, 0], [2, 0], [-1000, 0], [0, 0], [-1000, 0], [0, 0],\n",
    "]\n",
    "\n",
    "neuron_classifying_2 = [\n",
    "    [1, 0], [0, 0], [0, 0], [-100, 0], [0, 0], [0, 0], [-100, 0],\n",
    "]\n",
    "\n",
    "neuron_classifying_3 = [\n",
    "    [1, 0], [0, 0], [0, 0], [-100, 0], [1, 0], [-100, 0], [1, 0],\n",
    "]\n",
    "\n",
    "neuron_classifying_4 = [\n",
    "    [0, 0], [1, 0], [1, 0], [3, 0], [0, 0], [-100, 0], [0, 0],\n",
    "]\n",
    "\n",
    "neuron_classifying_5 = [\n",
    "    [1, 0], [0, 0], [0, 0], [1, 0], [-100, 0], [-100, 0], [1, 0],\n",
    "]\n",
    "\n",
    "neuron_classifying_6 = [\n",
    "    [1, 0], [0, 0], [0, 0], [1, 0], [-100, 0], [2, -1], [1, 0],\n",
    "]\n",
    "\n",
    "neuron_classifying_7 = [\n",
    "    [0, 0], [9, 0], [0, 0], [-100, 0], [0, 0], [-100, 0], [0, 0],\n",
    "]\n",
    "\n",
    "neuron_classifying_8 = [\n",
    "    [1, 0], [0, 0], [0, 0], [1, 0], [1, 0], [2, -1], [1, 0],\n",
    "]\n",
    "\n",
    "neuron_classifying_9 = [\n",
    "    [1, 0], [0, 0], [0, 0], [1, 0], [1, 0], [-100, 0], [1, 0],\n",
    "]\n",
    "\n",
    "output_layer_lists = [\n",
    "    neuron_classifying_0,\n",
    "    neuron_classifying_1,\n",
    "    neuron_classifying_2,\n",
    "    neuron_classifying_3,\n",
    "    neuron_classifying_4,\n",
    "    neuron_classifying_5,\n",
    "    neuron_classifying_6,\n",
    "    neuron_classifying_7,\n",
    "    neuron_classifying_8,\n",
    "    neuron_classifying_9,\n",
    "]\n",
    "\n",
    "# Build matrix form: W_manual_out (10x7), b_manual_out (10,)\n",
    "W_manual_out = np.zeros((10, hidden_size_manual))\n",
    "b_manual_out = np.zeros(10)\n",
    "\n",
    "for k, neuron in enumerate(output_layer_lists):\n",
    "    neuron = np.array(neuron)\n",
    "    W_manual_out[k] = neuron[:, 0]        # weights\n",
    "    b_manual_out[k] = neuron[:, 1].sum()  # sum of small biases (some are -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_manual(x):\n",
    "    \"\"\"Forward pass through your hand-designed feature network.\"\"\"\n",
    "    # 1) Hidden features: dot products with your masks\n",
    "    h_raw = np.dot(W_hand, x) + b_hand    # (7,)\n",
    "    h = relu(h_raw)\n",
    "\n",
    "    # 2) Output scores: your classification rules\n",
    "    z = np.dot(W_manual_out, h) + b_manual_out  # (10,)\n",
    "    probs = softmax(z)\n",
    "    return probs\n",
    "\n",
    "\n",
    "def predict_manual(x):\n",
    "    return int(np.argmax(forward_manual(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Manual, hand-crafted feature network ===\n",
      "Digit 0 -> predicted 0\n",
      "Digit 1 -> predicted 1\n",
      "Digit 2 -> predicted 8\n",
      "Digit 3 -> predicted 3\n",
      "Digit 4 -> predicted 4\n",
      "Digit 5 -> predicted 5\n",
      "Digit 6 -> predicted 6\n",
      "Digit 7 -> predicted 7\n",
      "Digit 8 -> predicted 8\n",
      "Digit 9 -> predicted 9\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Manual, hand-crafted feature network ===\")\n",
    "for i in range(num_samples):\n",
    "    pred = predict_manual(X[i])\n",
    "    print(f\"Digit {y[i]} -> predicted {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size_learned = 8\n",
    "output_size = 10\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "params = {\n",
    "    \"W1\": rng.normal(0, 0.1, size=(hidden_size_learned, input_size)),   # (8,15)\n",
    "    \"b1\": np.zeros(hidden_size_learned),                                # (8,)\n",
    "    \"W2\": rng.normal(0, 0.1, size=(output_size, hidden_size_learned)),  # (10,8)\n",
    "    \"b2\": np.zeros(output_size),                                        # (10,)\n",
    "}\n",
    "\n",
    "\n",
    "def forward_single_learned(x, params):\n",
    "    \"\"\"\n",
    "    Forward pass for ONE image in the learned network.\n",
    "      hidden_raw = W1 · x + b1\n",
    "      hidden_act = ReLU(hidden_raw)\n",
    "      output_raw = W2 · hidden_act + b2\n",
    "      probs = softmax(output_raw)\n",
    "    \"\"\"\n",
    "    W1, b1, W2, b2 = params[\"W1\"], params[\"b1\"], params[\"W2\"], params[\"b2\"]\n",
    "\n",
    "    z1 = np.dot(W1, x) + b1    # (8,)\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(W2, a1) + b2   # (10,)\n",
    "    probs = softmax(z2)\n",
    "\n",
    "    cache = {\"x\": x, \"z1\": z1, \"a1\": a1, \"z2\": z2, \"probs\": probs}\n",
    "    return probs, cache\n",
    "\n",
    "\n",
    "def predict_single_learned(x, params):\n",
    "    probs, _ = forward_single_learned(x, params)\n",
    "    return int(np.argmax(probs))\n",
    "\n",
    "\n",
    "def cross_entropy_loss(probs, true_label):\n",
    "    return -np.log(probs[true_label] + 1e-12)\n",
    "\n",
    "\n",
    "def train(X, y, params, learning_rate=0.1, epochs=1000, print_every=100):\n",
    "    num_samples = X.shape[0]\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        dW1 = np.zeros_like(params[\"W1\"])\n",
    "        db1 = np.zeros_like(params[\"b1\"])\n",
    "        dW2 = np.zeros_like(params[\"W2\"])\n",
    "        db2 = np.zeros_like(params[\"b2\"])\n",
    "\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            x_i = X[i]\n",
    "            y_i = y[i]\n",
    "\n",
    "            # ---------- Forward ----------\n",
    "            probs, cache = forward_single_learned(x_i, params)\n",
    "            total_loss += cross_entropy_loss(probs, y_i)\n",
    "\n",
    "            if np.argmax(probs) == y_i:\n",
    "                correct += 1\n",
    "\n",
    "            # ---------- Backward ----------\n",
    "            # Output layer gradient\n",
    "            dz2 = probs.copy()\n",
    "            dz2[y_i] -= 1            # dL/dz2 = probs - one_hot(y)\n",
    "\n",
    "            a1 = cache[\"a1\"]\n",
    "            dW2 += np.outer(dz2, a1)\n",
    "            db2 += dz2\n",
    "\n",
    "            # Backprop into hidden layer\n",
    "            W2 = params[\"W2\"]\n",
    "            dz1 = np.dot(W2.T, dz2)  # (8,)\n",
    "\n",
    "            # ReLU derivative\n",
    "            z1 = cache[\"z1\"]\n",
    "            dz1[z1 <= 0] = 0\n",
    "\n",
    "            x_vec = cache[\"x\"]\n",
    "            dW1 += np.outer(dz1, x_vec)\n",
    "            db1 += dz1\n",
    "\n",
    "        # Average gradients\n",
    "        dW1 /= num_samples\n",
    "        db1 /= num_samples\n",
    "        dW2 /= num_samples\n",
    "        db2 /= num_samples\n",
    "\n",
    "        # Gradient descent step\n",
    "        params[\"W1\"] -= learning_rate * dW1\n",
    "        params[\"b1\"] -= learning_rate * db1\n",
    "        params[\"W2\"] -= learning_rate * dW2\n",
    "        params[\"b2\"] -= learning_rate * db2\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == 1:\n",
    "            avg_loss = total_loss / num_samples\n",
    "            accuracy = correct / num_samples\n",
    "            print(f\"[Learned NN] Epoch {epoch:4d} | loss = {avg_loss:.4f} | acc = {accuracy*100:.1f}%\")\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Manual, hand-crafted feature network ===\n",
      "Digit 0 -> predicted 0\n",
      "Digit 1 -> predicted 1\n",
      "Digit 2 -> predicted 8\n",
      "Digit 3 -> predicted 3\n",
      "Digit 4 -> predicted 4\n",
      "Digit 5 -> predicted 5\n",
      "Digit 6 -> predicted 6\n",
      "Digit 7 -> predicted 7\n",
      "Digit 8 -> predicted 8\n",
      "Digit 9 -> predicted 9\n",
      "\n",
      "=== Learned network BEFORE training ===\n",
      "Digit 0 -> predicted 0\n",
      "Digit 1 -> predicted 1\n",
      "Digit 2 -> predicted 2\n",
      "Digit 3 -> predicted 3\n",
      "Digit 4 -> predicted 4\n",
      "Digit 5 -> predicted 5\n",
      "Digit 6 -> predicted 6\n",
      "Digit 7 -> predicted 7\n",
      "Digit 8 -> predicted 8\n",
      "Digit 9 -> predicted 9\n",
      "[Learned NN] Epoch    1 | loss = 0.0073 | acc = 100.0%\n",
      "[Learned NN] Epoch  100 | loss = 0.0068 | acc = 100.0%\n",
      "[Learned NN] Epoch  200 | loss = 0.0064 | acc = 100.0%\n",
      "[Learned NN] Epoch  300 | loss = 0.0060 | acc = 100.0%\n",
      "[Learned NN] Epoch  400 | loss = 0.0056 | acc = 100.0%\n",
      "[Learned NN] Epoch  500 | loss = 0.0053 | acc = 100.0%\n",
      "[Learned NN] Epoch  600 | loss = 0.0050 | acc = 100.0%\n",
      "[Learned NN] Epoch  700 | loss = 0.0047 | acc = 100.0%\n",
      "[Learned NN] Epoch  800 | loss = 0.0045 | acc = 100.0%\n",
      "[Learned NN] Epoch  900 | loss = 0.0043 | acc = 100.0%\n",
      "[Learned NN] Epoch 1000 | loss = 0.0041 | acc = 100.0%\n",
      "\n",
      "=== Learned network AFTER training ===\n",
      "Digit 0 -> predicted 0\n",
      "Digit 1 -> predicted 1\n",
      "Digit 2 -> predicted 2\n",
      "Digit 3 -> predicted 3\n",
      "Digit 4 -> predicted 4\n",
      "Digit 5 -> predicted 5\n",
      "Digit 6 -> predicted 6\n",
      "Digit 7 -> predicted 7\n",
      "Digit 8 -> predicted 8\n",
      "Digit 9 -> predicted 9\n",
      "True: 2 | Manual: 8 | Learned: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPEAAAGFCAYAAAAsDxVqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEvElEQVR4nO3aMW4DMRAEQa5x//8y/QQdZMhEn6piBpM0NuGstfYCsn5ODwD+RsQQJ2KIEzHEiRjiRAxxIoY4EUPcdffh3v6EwH+bmZdvXGKIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIu+4+nJlP7uDL7L1PT3gMlxjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjirrsP996f3AG8ySWGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGuOv0gKeZmdMT+DIuMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcRdpwc8zd779AQeZGZevnGJIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIU7EECdiiBMxxIkY4kQMcSKGOBFDnIghTsQQJ2KIEzHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcSJGOJEDHEihjgRQ5yIIW7WWvv0COB9LjHEiRjiRAxxIoY4EUOciCFOxBAnYogTMcT9AgFeERFYULF8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== Manual, hand-crafted feature network ===\")\n",
    "for i in range(num_samples):\n",
    "    pred = predict_manual(X[i])\n",
    "    print(f\"Digit {y[i]} -> predicted {pred}\")\n",
    "\n",
    "print(\"\\n=== Learned network BEFORE training ===\")\n",
    "for i in range(num_samples):\n",
    "    pred = predict_single_learned(X[i], params)\n",
    "    print(f\"Digit {y[i]} -> predicted {pred}\")\n",
    "\n",
    "# Train learned network\n",
    "params = train(X, y, params, learning_rate=0.1, epochs=1000, print_every=100)\n",
    "\n",
    "print(\"\\n=== Learned network AFTER training ===\")\n",
    "for i in range(num_samples):\n",
    "    pred = predict_single_learned(X[i], params)\n",
    "    print(f\"Digit {y[i]} -> predicted {pred}\")\n",
    "\n",
    "# Show an example digit visually\n",
    "test_index = 2\n",
    "print(f\"True: {y[test_index]} | Manual: {predict_manual(X[test_index])} | Learned: {predict_single_learned(X[test_index], params)}\")\n",
    "show_digit(digits[test_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
